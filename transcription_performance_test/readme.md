Various experiments were performed to determine the accuracy of multiple  commercial transcription models. 
As of now, 4000 English audios, were transcribed using commercial models (Azure, Watson, Google) and an open source model (Deepspeech)
Another 800 Arabic audios, were transcribed using another dataset using both (Azure and Google)
This folder contains the results obtained from the various experiments performed

## Transcription Procedure
Within the test.py script under this folder, there is a method named 
transcribe_audios(files_path, format, modal_code, type)
The method takes in different parameters as described below to identify which vendor ((Google, Azure, watson, deepspeech) 
and which language to transcribe. Once this is done, an API call is made to the respective vendor to transcribe the audio using the specified language modal.
For deepspeech, their pre-trained transcription model is used.
 
### transcribe_audios(files_path, format, modal_code, type)
#### inputs
This method takes in a the following parameters:
   - files_path: the path where the audios files are located
   - format: "mp3" or "wav"
   - modal_code: any of the modals supported by google, azure. The modal code can be obtained from 
    the following URL  https://cloud.google.com/speech-to-text/docs/languages
   - type: can be either "azure", "google", "watson", "deepspeech"
   - Not all vendors support transcription for all languages. This can be looked from the official documentation of each vendor
   
#### outputs
A csv file that contains the transcription results associated with each file is generated
This method saves the results generated by the specified modals and vendor (type) into a csv file. 
The csv file will be created after 10 transcriptions. Everytime 10 audios are transcribed, the csv file will be 
automatically saved

#### Prerequisite
- API keys for each vendor (Google, Azure, Watson) has to be correctly configured within the engine.py file of the transcription analysis folder
- An empty folder named "audio_files" have to be added to this folder. In case the format of the audio is not '.wav', 
  this folder will be used as a temporary location to automatically convert the audio files into wav
- For Deepspeech, the open source modal (around 4 GB) should be downloaded and added into the parent directory of the project

## Scores
Within each experiment performed, after prepossessing the sentences, the following was used to calculate similarity
   - The sentence similarity results reported within each file use two different methods
      * Word Error Rate (WER): *The value shown with the csv files are "1 - WER" which identify similarity rather than error
          - The following python package was used to calculate WER: 
          - For more info about(WER): https://en.wikipedia.org/wiki/Word_error_rate 
      * Similarity Score: 
          - The scores were calculated using suggestion from the following median article
                https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50
          - The implemetation "get_similarity_scores" is under the test.py file
          - In all the expirements made, this method reported less scores than (1 - WER)
      